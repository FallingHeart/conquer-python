# 微博文案分析

## 字段说明

在上一篇中，我们获取的文案数据主要有
```python
bid # blog ID
content # 文案内容 由于解析方式的问题，获取不是很准确，有时只获取到一个【，待处理
attitudes_count # 点赞数
reposts_count # 转发数
comments_count # 评论数
created_at # 发布时间 包括来源的字符串，待处理 微博时间处理见https://www.cnblogs.com/c-x-a/archive/2004/01/13/8508824.html
```

其中bid和created_at是属于识别信息，content属于内容信息，attitudes_count, reposts_count, comments_count属于互动信息。

我们可以对每项互动信息进行降序的排列，得到最多或者最少的TOP10或者TOP100，点赞、转发、评论的信息。

|互动|最多|最少|备注|
|-|-|-|-|
|点赞|2947681|0|最具正能量的新闻
|转发|20905464|0|最具传播力的新闻
|评论|5049183|0|最引发反响的新闻

但光是这样，对于整体的互动质量我们是没有办法评估的。我们可以根据适当的权重得到一个互动分来进行综合的比较。

## 数据获取

获取内容非空的数据就可以，不用把MongoDB内部的id带上。

```python
import pymongo

myclient = pymongo.MongoClient("mongodb://XXX:XXX@XXX.XXX.XXX.XXX:27017/")
mydb = myclient["weibo_DB"]

def get_content_list(name):
    result_list = []
    mycol = mydb["weibo_contents"]
    result = mycol.find({'content':{'$ne':''}},{'_id':0})
    for x in result:
        result_list.append(x)
    return result_list

content_list = get_content_list() # 现在无需传入name以区分来自哪个账号的微博
# content_list = get_content_list('rmrb') # 之后可以在存入数据时带上来源字段，传入该函数进行查找
# print(content_list)
```

## 数据分析

针对这个数据我们可以传入一个点赞、转发、评论的权重，默认值为`(1,3,1.5)`，依据张会兵, 何彩梅, 胡晓丽,等. 融合行为和语义的节点影响力分析[J]. 计算机工程与应用, 2017, 53(011):151-154.

```python
import pandas as pd

def content_analyze(content_list,weights=(1,3,1.5)):
    attitudes_weight,reposts_weight,comments_weight = weights
    df = pd.DataFrame(content_list)
    df['content_score'] = df['attitudes_count'].apply(int) * attitudes_weight + df['reposts_count'].apply(int) * reposts_weight + df['comments_count'].apply(int) * comments_weight
    df.sort_values(by=['content_score'],ascending=False,inplace=True)
    print(df.head())

content_analyze(content_list)
```

前五个结果是这样，这就是非凡的国家凝聚力啊，多难兴邦。

|attitudes_count|reposts_count|comments_count|content_score|created_at|notes|bid|
|-|-|-|-|-|-|-|
|900391|20905464|442807|64280993.5|2018-05-11 23:59:50|汶川10周年|GgdfidjFm|
|322999|17542451|50106|53025511.0|02月05日 15:37|新冠求助|IsNq8eiI5|
|117182|15084286|259071|46813289.5|2019-10-01 00:00:22|2019年国庆|I9o0Cmx2f|
|310464|14921470|140783|45286048.5|2018-10-01 07:00:03|2018年国庆|GBQGn7iv6|
|1178409|12100602|105860|37639005.0|01月23日 07:17|新冠武汉|IqLAOtwIF|

我们还可以根据特定的关键词筛选出一系列包含该词的文案进行词频分析，情感分析，时间序列的分析，以及文本分类，这里就不再赘述。